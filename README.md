# Procesamiento de Datos en Databricks 

Este repositorio contiene scripts y notebooks para procesar grandes volúmenes de datos en Databricks utilizando PySpark, además de ciertos ejercicios realizados en sqlite3

## Estructura del Proyecto
- `data/` → Datos de entrada y datos de salida en formatos CSV, JSON y Parquet.
- `scripts/` → Scripts en PySpark para limpieza y transformación de datos.

## Instalación y Configuración
### Requisitos
- Python 3.8+
- PySpark
- Databricks CLI (opcional)

